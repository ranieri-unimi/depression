{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReg(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 1),\n",
    "            nn.ReLU(),\n",
    "            # nn.LogSoftmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=.001, max_beta=.1):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.max_beta = max_beta\n",
    "\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_losses, train_loss=None):\n",
    "\n",
    "        if len(val_losses) < 2:\n",
    "            return\n",
    "\n",
    "        curr, prev = val_losses[-1], val_losses[-2]\n",
    "        is_delta = (\n",
    "            (prev - curr) < self.min_delta\n",
    "        )\n",
    "        is_beta = (\n",
    "            (curr - train_loss) > self.max_beta\n",
    "        )\n",
    "\n",
    "        if is_beta or is_delta:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.tolerance:\n",
    "            self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer:\n",
    "    def __init__(self, dataset, input_size, batch_size=32) -> None:\n",
    "        self.train_losses = list()\n",
    "        self.val_losses = list()\n",
    "        self.batch_size = batch_size\n",
    "        self.load_data(dataset)\n",
    "\n",
    "        self.model = MyReg(input_size=input_size)\n",
    "        # self.lossf = nn.CrossEntropyLoss()\n",
    "        self.lossf = nn.L1Loss()\n",
    "        self.optif = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=1e-3,\n",
    "        )\n",
    "        # self.optif = torch.optim.SGD(\n",
    "        #     self.model.parameters(),\n",
    "        #     lr=1e-3,\n",
    "        #     momentum=0.9,\n",
    "        # )\n",
    "\n",
    "    def load_data(self, dataset):\n",
    "        train_dt, val_dt, test_dt = random_split(dataset, [0.6, 0.2, 0.2])\n",
    "        self._train_dl = DataLoader(\n",
    "            train_dt,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        self._val_dl = DataLoader(val_dt, shuffle=True)\n",
    "        self._test_dl = DataLoader(test_dt, shuffle=True)\n",
    "\n",
    "    def _train_one(self):\n",
    "        epoch_loss = 0.0\n",
    "        for _, data in enumerate(self._train_dl):\n",
    "            inputs, targets = data\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            self.optif.zero_grad()\n",
    "            batch_loss = self.lossf(outputs[:, 0], targets)\n",
    "            batch_loss.backward()\n",
    "            self.optif.step()\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "        return epoch_loss / len(self._train_dl)\n",
    "\n",
    "    def _val_one(self):\n",
    "        epoch_loss = 0.0\n",
    "        for _, data in enumerate(self._val_dl):\n",
    "            inputs, target = data\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            loss = self.lossf(outputs[:, 0], target)\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / len(self._val_dl)\n",
    "\n",
    "    def fit(self, epochs=1, early_stopping=None):\n",
    "        for epoch in range(epochs):\n",
    "            t_loss = self._train_one()\n",
    "            self.train_losses.append(t_loss)\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            v_loss = self._val_one()\n",
    "            self.val_losses.append(v_loss)\n",
    "            print(f\"validation loss {v_loss:.3f} at epoch {epoch}\")\n",
    "\n",
    "            # early stopping\n",
    "            if early_stopping is not None:\n",
    "                early_stopping(self.val_losses, t_loss)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"early stopping...\")\n",
    "                    break\n",
    "\n",
    "    def gasp(self):\n",
    "        return [(self.model(inputs).item(), target.item()) for inputs, target in self._test_dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(predictions, labels):\n",
    "\n",
    "    yy = list(zip(predictions, labels))\n",
    "\n",
    "    P_TRUE = sum(labels)\n",
    "    P_HAT = sum(predictions)\n",
    "\n",
    "    TP = yy.count((1, 1))\n",
    "    TN = yy.count((0, 0))\n",
    "    N = len(yy)\n",
    "\n",
    "    precision = TP / P_HAT\n",
    "    recall = TP / P_TRUE\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    accuracy = (TP + TN) / N\n",
    "\n",
    "    print(f\"F1: {F1:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and mergin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = \"\"\n",
    "\n",
    "for filename in sorted([\"ds\", \"ts_hs\", \"ts_ht\"]):\n",
    "    with open(\n",
    "        Path(\"datasets\", \"task_0\", f\"{filename}.tsv\"), \"rt\", encoding=\"utf8\"\n",
    "    ) as f:\n",
    "        data_text += f.read()\n",
    "\n",
    "df = pd.read_csv(StringIO(data_text), sep=\"\\t\")\n",
    "df = df.drop_duplicates().reset_index(names=\"old_idx\").reset_index(names=\"new_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label = df.label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "to_liwc, categories = liwc.load_token_parser('dic/LIWC2007_English080730.dic')\n",
    "K = len(categories)\n",
    "\n",
    "kat_lookup = dict(zip(categories, range(K)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexDataset(Dataset):\n",
    "    def __init__(self, df, x_col=\"pp_text\", y_col=\"label\"):\n",
    "        self.df = df\n",
    "        self.x_col = x_col\n",
    "        self.y_col = y_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        counter = {k: 0 for k in categories}\n",
    "        for word in self.df.loc[idx, self.x_col].split():\n",
    "            for k in list(to_liwc(word)):\n",
    "                counter[k] += 1\n",
    "        X = (\n",
    "            np.array([counter[k] for k in categories]) / sum(counter.values())\n",
    "            if sum(counter.values())\n",
    "            else np.zeros(K)\n",
    "        )\n",
    "        # y = np.zeros(2)\n",
    "        # y[self.df.loc[idx, self.y_col]] = 1\n",
    "        y = self.df.loc[idx, self.y_col]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = LexDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(ds, 64)\n",
    "trainer.fit(\n",
    "    epochs=100,\n",
    "    early_stopping=EarlyStopping(\n",
    "        tolerance=4,\n",
    "        min_delta=0.0,\n",
    "        max_beta=0.1,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(trainer.train_losses)\n",
    "sns.lineplot(trainer.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = trainer.model.layers[0].weight[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, y_true = list(zip(*trainer.gasp()))\n",
    "y_hat = [1 if e>0.5 else 0 for e in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"ShreyaR/finetuned-roberta-depression\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ranieri-unimi/test-trainer\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = tokenizer_hf.batch_encode_plus(df.pp_text.to_list(), return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "y = torch.tensor(df.label).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(y), BATCH_SIZE):\n",
    "        input_ids = TH.input_ids[i : i + BATCH_SIZE, :]\n",
    "        result = model(input_ids, output_hidden_states=True)\n",
    "        cls_batch = result.hidden_states[-1][0, 0, :]\n",
    "        try:\n",
    "            X = torch.cat((X, cls_batch), 0)\n",
    "        except:\n",
    "            X = cls_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobDataset(Dataset):\n",
    "    def __init__(self, X_list, y_list):\n",
    "        self.X_list = X_list\n",
    "        self.y_list = y_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_list[idx], self.y_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = RobDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(ds, 768)\n",
    "trainer.fit(\n",
    "    epochs=100,\n",
    "    early_stopping=EarlyStopping(\n",
    "        tolerance=4,\n",
    "        min_delta=0.0,\n",
    "        max_beta=0.1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(trainer.train_losses)\n",
    "sns.lineplot(trainer.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_weights = trainer.model.layers[0].weight[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, y_true = list(zip(*trainer.gasp()))\n",
    "y_hat = [1 if e>0.5 else 0 for e in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_weights.shape, weights.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a03f457ba8100ed956a9dead337cf895f8e146c4ebc145127222e1f06f191e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
