{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1050'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2EAmV4NpLElz"
   },
   "outputs": [],
   "source": [
    "# !unzip datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erisk import erde_evaluation, erde_mem\n",
    "\n",
    "def erde(out_file, o):\n",
    "    erde_evaluation(\"datasets/task_1_depression/risk-golden-truth-test.txt\", out_file, o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dict(\n",
    "    pd.read_csv(\n",
    "        \"datasets/task_1_depression/risk-golden-truth-test.txt\",\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "    ).to_records(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/task_1_depression/depression_merged.csv\") # output by read.ipynb \n",
    "subjects = sorted(df.user.drop_duplicates().to_list())\n",
    "S = len(subjects)\n",
    "subject_lookup =  dict(zip(subjects, range(S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "liwc_parse, categories = liwc.load_token_parser('dic/LIWC2007_English080730.dic')\n",
    "K = len(categories)\n",
    "category_lookup = dict(zip(categories, range(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_liwc_token(text):\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0).lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate two time serie for each subject: one for text and one for lexical represetation of the former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lex_ts = dict()\n",
    "x_text_ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [02:20<00:00,  5.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for subj, dfi in tqdm(df.groupby(\"user\")):\n",
    "    x_lex_ts[subj] = list()\n",
    "    x_text_ts[subj] = list()\n",
    "\n",
    "    for n, text in enumerate(dfi.sort_values(\"date_time\").text):\n",
    "        categ_freq = np.zeros(K)\n",
    "        for t in to_liwc_token(text):\n",
    "            for m in liwc_parse(t):\n",
    "                k = category_lookup[m]\n",
    "                categ_freq[k] += 1\n",
    "\n",
    "        if not categ_freq.sum():\n",
    "            continue\n",
    "\n",
    "        categ_freq /= categ_freq.sum()\n",
    "\n",
    "        x_lex_ts[subj].append(categ_freq)\n",
    "        x_text_ts[subj].append(text.strip())\n",
    "    x_lex_ts[subj] = np.array(x_lex_ts[subj])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_LENGTH = 20\n",
    "# x_lex_ts = {k: v for k, v in x_lex_ts.items() if not v.shape[0] < MIN_LENGTH}\n",
    "# x_sign_lex_ts = {k: np.sign(v) for k, v in x_lex_ts.items()}\n",
    "# y_true = {k: v for k, v in y_true.items() if k in x_lex_ts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_and_store(X, y, test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    aaa_path = r\"./MLSTM_FCN/data/aaa/\"\n",
    "    np.save(aaa_path + 'X_train.npy', X_train)\n",
    "    np.save(aaa_path + 'y_train.npy', y_train)\n",
    "    np.save(aaa_path + 'X_test.npy', X_test)\n",
    "    np.save(aaa_path + 'y_test.npy', y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lexical time series, we classify them with a multivariate LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Shoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.zeros((len(y_true), 64, 2000)), []\n",
    "for i, subj in enumerate(y_true):\n",
    "    y.append(y_true[subj])\n",
    "    X_unpad = x_lex_ts[subj]\n",
    "\n",
    "    ts_len = X_unpad.T.shape[-1]\n",
    "    X[i, :, :ts_len] = X_unpad.T\n",
    "\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,:,:HORIZON]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spilt_and_store(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 5\n",
    "SHIFT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_frames = []\n",
    "y_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 820/820 [00:01<00:00, 735.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for subj, y in tqdm(y_true.items()):\n",
    "    ts = x_lex_ts[subj].T\n",
    "    for i in range(0, ts.shape[-1] - HORIZON, SHIFT):\n",
    "        pad = np.zeros((ts.shape[0], HORIZON))\n",
    "        ts_len = ts.shape[-1]\n",
    "        pad[:, :ts_len] = ts[:, i : i + HORIZON]\n",
    "        X_frames.append(pad)\n",
    "        y_frames.append(y)\n",
    "\n",
    "X_frames = np.array(X_frames)\n",
    "y_frames = np.array(y_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spilt_and_store(X_frames, y_frames, 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From h:\\git.riva\\venv37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From h:\\git.riva\\venv37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2023-01-25 14:27:39.110433: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "WARNING:tensorflow:From h:\\git.riva\\venv37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 5)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 5, 64)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 5, 128)       65664       permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 5, 128)       512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5, 128)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 8)         1024        reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1, 128)       1024        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 5, 128)       0           activation_1[0][0]               \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 5, 256)       164096      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 5, 256)       1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5, 256)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1, 16)        4096        reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1, 256)       4096        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 5, 256)       0           activation_2[0][0]               \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 5, 128)       98432       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 64, 5)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 128)       512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_lstm_1 (AttentionLSTM (None, 8)            728         masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5, 128)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 8)            0           attention_lstm_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]                  \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            274         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 341,482\n",
      "Trainable params: 340,458\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "Loading train / test dataset :  H:/git.riva/MLSTM_FCN/data/aaa/ H:/git.riva/MLSTM_FCN/data/aaa/\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  84208 Number of test samples :  21053\n",
      "Number of classes :  2\n",
      "Sequence length :  5\n",
      "Class weights :  [0.54052944 6.66835603]\n",
      "Train on 84208 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      " - 313s - loss: 0.2383 - accuracy: 0.9225 - f1_score: 0.9225 - val_loss: 0.2615 - val_accuracy: 0.9050 - val_f1_score: 0.8984\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.23831, saving model to ./weights/aaa_weights.h5\n",
      "Epoch 2/5\n",
      " - 389s - loss: 0.2256 - accuracy: 0.9255 - f1_score: 0.9255 - val_loss: 0.2698 - val_accuracy: 0.9050 - val_f1_score: 0.8984\n",
      "\n",
      "Epoch 00002: loss improved from 0.23831 to 0.22564, saving model to ./weights/aaa_weights.h5\n",
      "Epoch 3/5\n",
      " - 378s - loss: 0.2204 - accuracy: 0.9260 - f1_score: 0.9260 - val_loss: 0.2758 - val_accuracy: 0.9050 - val_f1_score: 0.8984\n",
      "\n",
      "Epoch 00003: loss improved from 0.22564 to 0.22036, saving model to ./weights/aaa_weights.h5\n",
      "Epoch 4/5\n",
      " - 373s - loss: 0.2145 - accuracy: 0.9265 - f1_score: 0.9265 - val_loss: 0.2882 - val_accuracy: 0.9050 - val_f1_score: 0.8984\n",
      "\n",
      "Epoch 00004: loss improved from 0.22036 to 0.21446, saving model to ./weights/aaa_weights.h5\n",
      "Epoch 5/5\n",
      " - 306s - loss: 0.2061 - accuracy: 0.9274 - f1_score: 0.9274 - val_loss: 0.3189 - val_accuracy: 0.9050 - val_f1_score: 0.8984\n",
      "\n",
      "Epoch 00005: loss improved from 0.21446 to 0.20606, saving model to ./weights/aaa_weights.h5\n",
      "Loading train / test dataset :  H:/git.riva/MLSTM_FCN/data/aaa/ H:/git.riva/MLSTM_FCN/data/aaa/\n",
      "Finished processing train dataset..\n",
      "Finished loading test dataset..\n",
      "\n",
      "Number of train samples :  84208 Number of test samples :  21053\n",
      "Number of classes :  2\n",
      "Sequence length :  5\n",
      "\n",
      "Evaluating : \n",
      "\n",
      "   64/20853 [..............................] - ETA: 2:26\n",
      "  128/20853 [..............................] - ETA: 1:23\n",
      "  192/20853 [..............................] - ETA: 1:03\n",
      "  256/20853 [..............................] - ETA: 53s \n",
      "  320/20853 [..............................] - ETA: 46s\n",
      "  384/20853 [..............................] - ETA: 41s\n",
      "  448/20853 [..............................] - ETA: 38s\n",
      "  512/20853 [..............................] - ETA: 36s\n",
      "  576/20853 [..............................] - ETA: 34s\n",
      "  640/20853 [..............................] - ETA: 33s\n",
      "  704/20853 [>.............................] - ETA: 31s\n",
      "  768/20853 [>.............................] - ETA: 31s\n",
      "  832/20853 [>.............................] - ETA: 30s\n",
      "  896/20853 [>.............................] - ETA: 29s\n",
      "  960/20853 [>.............................] - ETA: 28s\n",
      " 1024/20853 [>.............................] - ETA: 28s\n",
      " 1088/20853 [>.............................] - ETA: 27s\n",
      " 1152/20853 [>.............................] - ETA: 27s\n",
      " 1216/20853 [>.............................] - ETA: 26s\n",
      " 1280/20853 [>.............................] - ETA: 26s\n",
      " 1344/20853 [>.............................] - ETA: 26s\n",
      " 1408/20853 [=>............................] - ETA: 26s\n",
      " 1536/20853 [=>............................] - ETA: 25s\n",
      " 1600/20853 [=>............................] - ETA: 25s\n",
      " 1664/20853 [=>............................] - ETA: 24s\n",
      " 1728/20853 [=>............................] - ETA: 24s\n",
      " 1792/20853 [=>............................] - ETA: 24s\n",
      " 1856/20853 [=>............................] - ETA: 23s\n",
      " 1920/20853 [=>............................] - ETA: 23s\n",
      " 1984/20853 [=>............................] - ETA: 23s\n",
      " 2048/20853 [=>............................] - ETA: 23s\n",
      " 2112/20853 [==>...........................] - ETA: 23s\n",
      " 2176/20853 [==>...........................] - ETA: 23s\n",
      " 2240/20853 [==>...........................] - ETA: 22s\n",
      " 2304/20853 [==>...........................] - ETA: 22s\n",
      " 2368/20853 [==>...........................] - ETA: 22s\n",
      " 2432/20853 [==>...........................] - ETA: 22s\n",
      " 2496/20853 [==>...........................] - ETA: 22s\n",
      " 2560/20853 [==>...........................] - ETA: 22s\n",
      " 2624/20853 [==>...........................] - ETA: 21s\n",
      " 2688/20853 [==>...........................] - ETA: 21s\n",
      " 2752/20853 [==>...........................] - ETA: 21s\n",
      " 2816/20853 [===>..........................] - ETA: 21s\n",
      " 2880/20853 [===>..........................] - ETA: 21s\n",
      " 2944/20853 [===>..........................] - ETA: 21s\n",
      " 3008/20853 [===>..........................] - ETA: 21s\n",
      " 3072/20853 [===>..........................] - ETA: 21s\n",
      " 3136/20853 [===>..........................] - ETA: 20s\n",
      " 3200/20853 [===>..........................] - ETA: 20s\n",
      " 3264/20853 [===>..........................] - ETA: 20s\n",
      " 3328/20853 [===>..........................] - ETA: 20s\n",
      " 3392/20853 [===>..........................] - ETA: 20s\n",
      " 3456/20853 [===>..........................] - ETA: 20s\n",
      " 3520/20853 [====>.........................] - ETA: 20s\n",
      " 3584/20853 [====>.........................] - ETA: 19s\n",
      " 3648/20853 [====>.........................] - ETA: 19s\n",
      " 3712/20853 [====>.........................] - ETA: 19s\n",
      " 3776/20853 [====>.........................] - ETA: 19s\n",
      " 3840/20853 [====>.........................] - ETA: 19s\n",
      " 3904/20853 [====>.........................] - ETA: 19s\n",
      " 3968/20853 [====>.........................] - ETA: 19s\n",
      " 4032/20853 [====>.........................] - ETA: 19s\n",
      " 4096/20853 [====>.........................] - ETA: 19s\n",
      " 4160/20853 [====>.........................] - ETA: 19s\n",
      " 4224/20853 [=====>........................] - ETA: 18s\n",
      " 4288/20853 [=====>........................] - ETA: 18s\n",
      " 4352/20853 [=====>........................] - ETA: 18s\n",
      " 4416/20853 [=====>........................] - ETA: 18s\n",
      " 4480/20853 [=====>........................] - ETA: 18s\n",
      " 4544/20853 [=====>........................] - ETA: 18s\n",
      " 4608/20853 [=====>........................] - ETA: 18s\n",
      " 4672/20853 [=====>........................] - ETA: 18s\n",
      " 4736/20853 [=====>........................] - ETA: 18s\n",
      " 4800/20853 [=====>........................] - ETA: 18s\n",
      " 4864/20853 [=====>........................] - ETA: 17s\n",
      " 4928/20853 [======>.......................] - ETA: 17s\n",
      " 4992/20853 [======>.......................] - ETA: 17s\n",
      " 5056/20853 [======>.......................] - ETA: 17s\n",
      " 5120/20853 [======>.......................] - ETA: 17s\n",
      " 5184/20853 [======>.......................] - ETA: 17s\n",
      " 5248/20853 [======>.......................] - ETA: 17s\n",
      " 5312/20853 [======>.......................] - ETA: 17s\n",
      " 5376/20853 [======>.......................] - ETA: 17s\n",
      " 5440/20853 [======>.......................] - ETA: 17s\n",
      " 5504/20853 [======>.......................] - ETA: 17s\n",
      " 5568/20853 [=======>......................] - ETA: 17s\n",
      " 5632/20853 [=======>......................] - ETA: 16s\n",
      " 5696/20853 [=======>......................] - ETA: 16s\n",
      " 5760/20853 [=======>......................] - ETA: 16s\n",
      " 5824/20853 [=======>......................] - ETA: 16s\n",
      " 5888/20853 [=======>......................] - ETA: 16s\n",
      " 5952/20853 [=======>......................] - ETA: 16s\n",
      " 6016/20853 [=======>......................] - ETA: 16s\n",
      " 6080/20853 [=======>......................] - ETA: 16s\n",
      " 6144/20853 [=======>......................] - ETA: 16s\n",
      " 6208/20853 [=======>......................] - ETA: 16s\n",
      " 6272/20853 [========>.....................] - ETA: 16s\n",
      " 6336/20853 [========>.....................] - ETA: 16s\n",
      " 6400/20853 [========>.....................] - ETA: 16s\n",
      " 6464/20853 [========>.....................] - ETA: 15s\n",
      " 6528/20853 [========>.....................] - ETA: 15s\n",
      " 6592/20853 [========>.....................] - ETA: 15s\n",
      " 6656/20853 [========>.....................] - ETA: 15s\n",
      " 6720/20853 [========>.....................] - ETA: 15s\n",
      " 6784/20853 [========>.....................] - ETA: 15s\n",
      " 6848/20853 [========>.....................] - ETA: 15s\n",
      " 6912/20853 [========>.....................] - ETA: 15s\n",
      " 6976/20853 [=========>....................] - ETA: 15s\n",
      " 7040/20853 [=========>....................] - ETA: 15s\n",
      " 7104/20853 [=========>....................] - ETA: 15s\n",
      " 7168/20853 [=========>....................] - ETA: 15s\n",
      " 7232/20853 [=========>....................] - ETA: 15s\n",
      " 7296/20853 [=========>....................] - ETA: 14s\n",
      " 7360/20853 [=========>....................] - ETA: 14s\n",
      " 7424/20853 [=========>....................] - ETA: 14s\n",
      " 7488/20853 [=========>....................] - ETA: 14s\n",
      " 7552/20853 [=========>....................] - ETA: 14s\n",
      " 7616/20853 [=========>....................] - ETA: 14s\n",
      " 7680/20853 [==========>...................] - ETA: 14s\n",
      " 7744/20853 [==========>...................] - ETA: 14s\n",
      " 7808/20853 [==========>...................] - ETA: 14s\n",
      " 7872/20853 [==========>...................] - ETA: 14s\n",
      " 7936/20853 [==========>...................] - ETA: 14s\n",
      " 8000/20853 [==========>...................] - ETA: 14s\n",
      " 8064/20853 [==========>...................] - ETA: 14s\n",
      " 8128/20853 [==========>...................] - ETA: 13s\n",
      " 8192/20853 [==========>...................] - ETA: 13s\n",
      " 8256/20853 [==========>...................] - ETA: 13s\n",
      " 8320/20853 [==========>...................] - ETA: 13s\n",
      " 8384/20853 [===========>..................] - ETA: 13s\n",
      " 8448/20853 [===========>..................] - ETA: 13s\n",
      " 8512/20853 [===========>..................] - ETA: 13s\n",
      " 8576/20853 [===========>..................] - ETA: 13s\n",
      " 8640/20853 [===========>..................] - ETA: 13s\n",
      " 8704/20853 [===========>..................] - ETA: 13s\n",
      " 8768/20853 [===========>..................] - ETA: 13s\n",
      " 8832/20853 [===========>..................] - ETA: 13s\n",
      " 8896/20853 [===========>..................] - ETA: 13s\n",
      " 8960/20853 [===========>..................] - ETA: 13s\n",
      " 9024/20853 [===========>..................] - ETA: 12s\n",
      " 9088/20853 [============>.................] - ETA: 12s\n",
      " 9152/20853 [============>.................] - ETA: 12s\n",
      " 9216/20853 [============>.................] - ETA: 12s\n",
      " 9280/20853 [============>.................] - ETA: 12s\n",
      " 9344/20853 [============>.................] - ETA: 12s\n",
      " 9408/20853 [============>.................] - ETA: 12s\n",
      " 9472/20853 [============>.................] - ETA: 12s\n",
      " 9536/20853 [============>.................] - ETA: 12s\n",
      " 9600/20853 [============>.................] - ETA: 12s\n",
      " 9664/20853 [============>.................] - ETA: 12s\n",
      " 9728/20853 [============>.................] - ETA: 12s\n",
      " 9792/20853 [=============>................] - ETA: 12s\n",
      " 9856/20853 [=============>................] - ETA: 12s\n",
      " 9920/20853 [=============>................] - ETA: 11s\n",
      " 9984/20853 [=============>................] - ETA: 11s\n",
      "10048/20853 [=============>................] - ETA: 11s\n",
      "10112/20853 [=============>................] - ETA: 11s\n",
      "10176/20853 [=============>................] - ETA: 11s\n",
      "10240/20853 [=============>................] - ETA: 11s\n",
      "10304/20853 [=============>................] - ETA: 11s\n",
      "10368/20853 [=============>................] - ETA: 11s\n",
      "10432/20853 [==============>...............] - ETA: 11s\n",
      "10496/20853 [==============>...............] - ETA: 11s\n",
      "10560/20853 [==============>...............] - ETA: 11s\n",
      "10624/20853 [==============>...............] - ETA: 11s\n",
      "10688/20853 [==============>...............] - ETA: 11s\n",
      "10752/20853 [==============>...............] - ETA: 11s\n",
      "10816/20853 [==============>...............] - ETA: 10s\n",
      "10880/20853 [==============>...............] - ETA: 10s\n",
      "10944/20853 [==============>...............] - ETA: 10s\n",
      "11008/20853 [==============>...............] - ETA: 10s\n",
      "11072/20853 [==============>...............] - ETA: 10s\n",
      "11136/20853 [===============>..............] - ETA: 10s\n",
      "11200/20853 [===============>..............] - ETA: 10s\n",
      "11264/20853 [===============>..............] - ETA: 10s\n",
      "11328/20853 [===============>..............] - ETA: 10s\n",
      "11392/20853 [===============>..............] - ETA: 10s\n",
      "11456/20853 [===============>..............] - ETA: 10s\n",
      "11520/20853 [===============>..............] - ETA: 10s\n",
      "11584/20853 [===============>..............] - ETA: 10s\n",
      "11648/20853 [===============>..............] - ETA: 9s \n",
      "11712/20853 [===============>..............] - ETA: 9s\n",
      "11776/20853 [===============>..............] - ETA: 9s\n",
      "11840/20853 [================>.............] - ETA: 9s\n",
      "11904/20853 [================>.............] - ETA: 9s\n",
      "11968/20853 [================>.............] - ETA: 9s\n",
      "12032/20853 [================>.............] - ETA: 9s\n",
      "12096/20853 [================>.............] - ETA: 9s\n",
      "12160/20853 [================>.............] - ETA: 9s\n",
      "12224/20853 [================>.............] - ETA: 9s\n",
      "12288/20853 [================>.............] - ETA: 9s\n",
      "12352/20853 [================>.............] - ETA: 9s\n",
      "12416/20853 [================>.............] - ETA: 9s\n",
      "12480/20853 [================>.............] - ETA: 9s\n",
      "12544/20853 [=================>............] - ETA: 8s\n",
      "12608/20853 [=================>............] - ETA: 8s\n",
      "12672/20853 [=================>............] - ETA: 8s\n",
      "12736/20853 [=================>............] - ETA: 8s\n",
      "12800/20853 [=================>............] - ETA: 8s\n",
      "12864/20853 [=================>............] - ETA: 8s\n",
      "12928/20853 [=================>............] - ETA: 8s\n",
      "12992/20853 [=================>............] - ETA: 8s\n",
      "13056/20853 [=================>............] - ETA: 8s\n",
      "13120/20853 [=================>............] - ETA: 8s\n",
      "13184/20853 [=================>............] - ETA: 8s\n",
      "13248/20853 [==================>...........] - ETA: 8s\n",
      "13312/20853 [==================>...........] - ETA: 8s\n",
      "13376/20853 [==================>...........] - ETA: 8s\n",
      "13440/20853 [==================>...........] - ETA: 8s\n",
      "13504/20853 [==================>...........] - ETA: 7s\n",
      "13568/20853 [==================>...........] - ETA: 7s\n",
      "13632/20853 [==================>...........] - ETA: 7s\n",
      "13696/20853 [==================>...........] - ETA: 7s\n",
      "13760/20853 [==================>...........] - ETA: 7s\n",
      "13824/20853 [==================>...........] - ETA: 7s\n",
      "13888/20853 [==================>...........] - ETA: 7s\n",
      "13952/20853 [===================>..........] - ETA: 7s\n",
      "14016/20853 [===================>..........] - ETA: 7s\n",
      "14080/20853 [===================>..........] - ETA: 7s\n",
      "14144/20853 [===================>..........] - ETA: 7s\n",
      "14208/20853 [===================>..........] - ETA: 7s\n",
      "14272/20853 [===================>..........] - ETA: 7s\n",
      "14336/20853 [===================>..........] - ETA: 7s\n",
      "14400/20853 [===================>..........] - ETA: 6s\n",
      "14464/20853 [===================>..........] - ETA: 6s\n",
      "14528/20853 [===================>..........] - ETA: 6s\n",
      "14592/20853 [===================>..........] - ETA: 6s\n",
      "14656/20853 [====================>.........] - ETA: 6s\n",
      "14720/20853 [====================>.........] - ETA: 6s\n",
      "14784/20853 [====================>.........] - ETA: 6s\n",
      "14848/20853 [====================>.........] - ETA: 6s\n",
      "14912/20853 [====================>.........] - ETA: 6s\n",
      "14976/20853 [====================>.........] - ETA: 6s\n",
      "15040/20853 [====================>.........] - ETA: 6s\n",
      "15104/20853 [====================>.........] - ETA: 6s\n",
      "15168/20853 [====================>.........] - ETA: 6s\n",
      "15232/20853 [====================>.........] - ETA: 6s\n",
      "15296/20853 [=====================>........] - ETA: 5s\n",
      "15360/20853 [=====================>........] - ETA: 5s\n",
      "15424/20853 [=====================>........] - ETA: 5s\n",
      "15488/20853 [=====================>........] - ETA: 5s\n",
      "15552/20853 [=====================>........] - ETA: 5s\n",
      "15616/20853 [=====================>........] - ETA: 5s\n",
      "15680/20853 [=====================>........] - ETA: 5s\n",
      "15744/20853 [=====================>........] - ETA: 5s\n",
      "15808/20853 [=====================>........] - ETA: 5s\n",
      "15872/20853 [=====================>........] - ETA: 5s\n",
      "15936/20853 [=====================>........] - ETA: 5s\n",
      "16000/20853 [======================>.......] - ETA: 5s\n",
      "16064/20853 [======================>.......] - ETA: 5s\n",
      "16128/20853 [======================>.......] - ETA: 5s\n",
      "16192/20853 [======================>.......] - ETA: 5s\n",
      "16256/20853 [======================>.......] - ETA: 4s\n",
      "16320/20853 [======================>.......] - ETA: 4s\n",
      "16384/20853 [======================>.......] - ETA: 4s\n",
      "16448/20853 [======================>.......] - ETA: 4s\n",
      "16512/20853 [======================>.......] - ETA: 4s\n",
      "16576/20853 [======================>.......] - ETA: 4s\n",
      "16640/20853 [======================>.......] - ETA: 4s\n",
      "16704/20853 [=======================>......] - ETA: 4s\n",
      "16768/20853 [=======================>......] - ETA: 4s\n",
      "16832/20853 [=======================>......] - ETA: 4s\n",
      "16896/20853 [=======================>......] - ETA: 4s\n",
      "16960/20853 [=======================>......] - ETA: 4s\n",
      "17024/20853 [=======================>......] - ETA: 4s\n",
      "17088/20853 [=======================>......] - ETA: 4s\n",
      "17152/20853 [=======================>......] - ETA: 3s\n",
      "17216/20853 [=======================>......] - ETA: 3s\n",
      "17280/20853 [=======================>......] - ETA: 3s\n",
      "17344/20853 [=======================>......] - ETA: 3s\n",
      "17408/20853 [========================>.....] - ETA: 3s\n",
      "17472/20853 [========================>.....] - ETA: 3s\n",
      "17536/20853 [========================>.....] - ETA: 3s\n",
      "17600/20853 [========================>.....] - ETA: 3s\n",
      "17664/20853 [========================>.....] - ETA: 3s\n",
      "17728/20853 [========================>.....] - ETA: 3s\n",
      "17792/20853 [========================>.....] - ETA: 3s\n",
      "17856/20853 [========================>.....] - ETA: 3s\n",
      "17920/20853 [========================>.....] - ETA: 3s\n",
      "17984/20853 [========================>.....] - ETA: 3s\n",
      "18048/20853 [========================>.....] - ETA: 3s\n",
      "18112/20853 [=========================>....] - ETA: 2s\n",
      "18176/20853 [=========================>....] - ETA: 2s\n",
      "18240/20853 [=========================>....] - ETA: 2s\n",
      "18304/20853 [=========================>....] - ETA: 2s\n",
      "18368/20853 [=========================>....] - ETA: 2s\n",
      "18432/20853 [=========================>....] - ETA: 2s\n",
      "18496/20853 [=========================>....] - ETA: 2s\n",
      "18560/20853 [=========================>....] - ETA: 2s\n",
      "18624/20853 [=========================>....] - ETA: 2s\n",
      "18688/20853 [=========================>....] - ETA: 2s\n",
      "18752/20853 [=========================>....] - ETA: 2s\n",
      "18816/20853 [==========================>...] - ETA: 2s\n",
      "18880/20853 [==========================>...] - ETA: 2s\n",
      "18944/20853 [==========================>...] - ETA: 2s\n",
      "19008/20853 [==========================>...] - ETA: 1s\n",
      "19072/20853 [==========================>...] - ETA: 1s\n",
      "19136/20853 [==========================>...] - ETA: 1s\n",
      "19200/20853 [==========================>...] - ETA: 1s\n",
      "19264/20853 [==========================>...] - ETA: 1s\n",
      "19328/20853 [==========================>...] - ETA: 1s\n",
      "19392/20853 [==========================>...] - ETA: 1s\n",
      "19456/20853 [==========================>...] - ETA: 1s\n",
      "19520/20853 [===========================>..] - ETA: 1s\n",
      "19584/20853 [===========================>..] - ETA: 1s\n",
      "19648/20853 [===========================>..] - ETA: 1s\n",
      "19712/20853 [===========================>..] - ETA: 1s\n",
      "19776/20853 [===========================>..] - ETA: 1s\n",
      "19840/20853 [===========================>..] - ETA: 1s\n",
      "19904/20853 [===========================>..] - ETA: 1s\n",
      "19968/20853 [===========================>..] - ETA: 0s\n",
      "20032/20853 [===========================>..] - ETA: 0s\n",
      "20096/20853 [===========================>..] - ETA: 0s\n",
      "20160/20853 [============================>.] - ETA: 0s\n",
      "20224/20853 [============================>.] - ETA: 0s\n",
      "20288/20853 [============================>.] - ETA: 0s\n",
      "20352/20853 [============================>.] - ETA: 0s\n",
      "20416/20853 [============================>.] - ETA: 0s\n",
      "20480/20853 [============================>.] - ETA: 0s\n",
      "20544/20853 [============================>.] - ETA: 0s\n",
      "20608/20853 [============================>.] - ETA: 0s\n",
      "20672/20853 [============================>.] - ETA: 0s\n",
      "20736/20853 [============================>.] - ETA: 0s\n",
      "20800/20853 [============================>.] - ETA: 0s\n",
      "20853/20853 [==============================] - 22s 1ms/step\n",
      "\n",
      "Final Accuracy :  0.9257660508155823\n",
      "Final F1 score :  0.925795316696167\n"
     ]
    }
   ],
   "source": [
    "!.\\venv37\\Scripts\\python.exe .\\MLSTM_FCN\\aaa_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn_hat, y_nn_test = pickle.load(open(\"m_lstm_predictions.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global ERDE(50): 0.0689548600001629\n",
      "F1: 0.96\n",
      "Precision: 0.92\n",
      "Recall: 1.00\n"
     ]
    }
   ],
   "source": [
    "erde_mem(\n",
    "    [int(i) for i in (y_nn_hat[:, 0] * 2) // 1],\n",
    "    [int(i) for i in y_nn_test[:, 0]],\n",
    "    [HORIZON] * y_nn_test.shape[0],\n",
    "    50,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inspect model weights?\n",
    "- a smaller version?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROLLING VOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one shot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pWsLMHCLQPG"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlGSklPYIsOz"
   },
   "outputs": [],
   "source": [
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"ShreyaR/finetuned-roberta-depression\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ranieri-unimi/test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_text):\n",
    "    input_ids = tokenizer_hf.encode(input_text, return_tensors='pt')\n",
    "    if input_ids.shape[-1] > 512:\n",
    "        input_ids = torch.cat([input_ids[:,:511], input_ids[:,-1:]], dim=1)\n",
    "    output = model(input_ids)[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    return pred_label.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(input_texts):\n",
    "    input_ids = tokenizer_hf.batch_encode_plus(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    output = model(input_ids[\"input_ids\"])[0]\n",
    "    _, pred_labels = output.max(1)\n",
    "    return pred_labels.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_hat = dict()\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "for subj, ts in tqdm(x_text_ts.items()):\n",
    "    T = len(ts)\n",
    "    for i in range(0, T, BATCH_SIZE):\n",
    "        predictions = batch_predict(ts[i : i + BATCH_SIZE])\n",
    "        try:\n",
    "            yt_hat[subj] = np.concatenate((yt_hat[subj], predictions), axis=None)\n",
    "        except:\n",
    "            yt_hat[subj] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yt_hat = dict()\n",
    "\n",
    "# for subj, ts in tqdm(x_text_ts.items()):\n",
    "#     yt_hat[subj] = list()\n",
    "#     for text in ts:\n",
    "#         p = predict(text)\n",
    "#         yt_hat[subj].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_hat = pickle.load(open(\"stash/all.prediction.pkl\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING = 13\n",
    "YIELD_THRESHOLD = 0.990\n",
    "weight_window = np.ones(ROLLING) / ROLLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = list()\n",
    "for subj, ts in tqdm(yt_hat.items()):\n",
    "    T = len(ts)\n",
    "    result = 0\n",
    "    pred_window = np.array(yt_hat[subj][: ROLLING - 1] + [0])\n",
    "    for t in range(ROLLING - 1, T):\n",
    "        pred_window[t % ROLLING] = yt_hat[subj][t]\n",
    "        score = np.dot(pred_window, weight_window)\n",
    "        if score >= YIELD_THRESHOLD:\n",
    "            result = 1\n",
    "            break\n",
    "    y_hat.append([subj, result, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_mem(*list(zip(*[(y, y_true[subj], t) for subj, y, t in y_hat])), 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hypertuning?\n",
    "- explore refining methods on binary ts? HMM: https://pure.unileoben.ac.at/portal/files/1073252/Improving_Time_Series_Classification_Using_Hidden_Markov_Models.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_hat).to_csv(\"rollig_vote_results.csv\", index=False, header=None, sep=\" \")\n",
    "# erde(\"rollig_vote_results.csv\", 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIC SUNT LEONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0/0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from  statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = categories.index(\"family\")\n",
    "s = \"subject3414\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = x_sign_lex_ts[s][:50,k]\n",
    "ts = ma(ts, 3)\n",
    "ts = x_lex_ts[s][:50,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts, order=(1,3,2))\n",
    "results = model.fit()\n",
    "plt.plot(ts)\n",
    "plt.plot(results.fittedvalues[1:], color='red', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a03f457ba8100ed956a9dead337cf895f8e146c4ebc145127222e1f06f191e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
