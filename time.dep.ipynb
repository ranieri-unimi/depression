{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAA_PATH = r\"./MLSTM_FCN/data/aaa/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EAmV4NpLElz"
   },
   "outputs": [],
   "source": [
    "# !unzip datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from erisk import erde_evaluation, erde_mem\n",
    "\n",
    "def erde(out_file, o):\n",
    "    erde_evaluation(\"datasets/task_1_depression/risk-golden-truth-test.txt\", out_file, o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = dict(\n",
    "    pd.read_csv(\n",
    "        \"datasets/task_1_depression/risk-golden-truth-test.txt\",\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "    ).to_records(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/task_1_depression/depression_merged.csv\") # output by read.ipynb \n",
    "subjects = sorted(df.user.drop_duplicates().to_list())\n",
    "S = len(subjects)\n",
    "subject_lookup =  dict(zip(subjects, range(S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "liwc_parse, categories = liwc.load_token_parser('dic/LIWC2007_English080730.dic')\n",
    "K = len(categories)\n",
    "category_lookup = dict(zip(categories, range(K)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_liwc_token(text):\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0).lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate two time serie for each subject: one for text and one for lexical represetation of the former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lex_ts = dict()\n",
    "x_text_ts = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, dfi in tqdm(df.groupby(\"user\")):\n",
    "    x_lex_ts[subj] = list()\n",
    "    x_text_ts[subj] = list()\n",
    "\n",
    "    for n, text in enumerate(dfi.sort_values(\"date_time\").text):\n",
    "        categ_freq = np.zeros(K)\n",
    "        for t in to_liwc_token(text):\n",
    "            for m in liwc_parse(t):\n",
    "                k = category_lookup[m]\n",
    "                categ_freq[k] += 1\n",
    "\n",
    "        if not categ_freq.sum():\n",
    "            continue\n",
    "\n",
    "        categ_freq /= categ_freq.sum()\n",
    "\n",
    "        x_lex_ts[subj].append(categ_freq)\n",
    "        x_text_ts[subj].append(text.strip())\n",
    "    x_lex_ts[subj] = np.array(x_lex_ts[subj])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_LENGTH = 20\n",
    "# x_lex_ts = {k: v for k, v in x_lex_ts.items() if not v.shape[0] < MIN_LENGTH}\n",
    "# x_sign_lex_ts = {k: np.sign(v) for k, v in x_lex_ts.items()}\n",
    "# y_true = {k: v for k, v in y_true.items() if k in x_lex_ts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_and_store(X, y, test_size=0.3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    AAA_PATH = r\"./MLSTM_FCN/data/aaa/\"\n",
    "    np.save(AAA_PATH + 'X_train.npy', X_train)\n",
    "    np.save(AAA_PATH + 'y_train.npy', y_train)\n",
    "    np.save(AAA_PATH + 'X_test.npy', X_test)\n",
    "    np.save(AAA_PATH + 'y_test.npy', y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lexical time series, we classify them with a multivariate LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Shoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.zeros((len(y_true), 64, 2000)), []\n",
    "for i, subj in enumerate(y_true):\n",
    "    y.append(y_true[subj])\n",
    "    X_unpad = x_lex_ts[subj]\n",
    "\n",
    "    ts_len = X_unpad.T.shape[-1]\n",
    "    X[i, :, :ts_len] = X_unpad.T\n",
    "\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:,:,:HORIZON]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spilt_and_store(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 1\n",
    "SHIFT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_frames = []\n",
    "y_frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj, y in tqdm(y_true.items()):\n",
    "    ts = x_lex_ts[subj].T\n",
    "    for i in range(0, ts.shape[-1] - HORIZON, SHIFT):\n",
    "        pad = np.zeros((ts.shape[0], HORIZON))\n",
    "        ts_len = ts.shape[-1]\n",
    "        pad[:, :ts_len] = ts[:, i : i + HORIZON]\n",
    "        X_frames.append(pad)\n",
    "        y_frames.append(y)\n",
    "\n",
    "X_frames = np.array(X_frames)\n",
    "y_frames = np.array(y_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spilt_and_store(X_frames, y_frames, 0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!.\\venv37\\Scripts\\python.exe .\\MLSTM_FCN\\aaa_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn_hat, y_nn_test = pickle.load(open(\"m_lstm_predictions.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_mem(\n",
    "    [int(i) for i in (y_nn_hat[:, 0] * 2) // 1],\n",
    "    [int(i) for i in y_nn_test[:, 0]],\n",
    "    [HORIZON] * y_nn_test.shape[0],\n",
    "    50,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- inspect model weights?\n",
    "- a smaller version?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROLLING VOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one shot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pWsLMHCLQPG"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pandas transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlGSklPYIsOz"
   },
   "outputs": [],
   "source": [
    "tokenizer_hf = AutoTokenizer.from_pretrained(\"ShreyaR/finetuned-roberta-depression\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ranieri-unimi/test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_text):\n",
    "    input_ids = tokenizer_hf.encode(input_text, return_tensors='pt')\n",
    "    if input_ids.shape[-1] > 512:\n",
    "        input_ids = torch.cat([input_ids[:,:511], input_ids[:,-1:]], dim=1)\n",
    "    output = model(input_ids)[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    return pred_label.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(input_texts):\n",
    "    input_ids = tokenizer_hf.batch_encode_plus(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    output = model(input_ids[\"input_ids\"])[0]\n",
    "    _, pred_labels = output.max(1)\n",
    "    return pred_labels.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_hat = dict()\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "for subj, ts in tqdm(x_text_ts.items()):\n",
    "    T = len(ts)\n",
    "    for i in range(0, T, BATCH_SIZE):\n",
    "        predictions = batch_predict(ts[i : i + BATCH_SIZE])\n",
    "        try:\n",
    "            yt_hat[subj] = np.concatenate((yt_hat[subj], predictions), axis=None)\n",
    "        except:\n",
    "            yt_hat[subj] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yt_hat = dict()\n",
    "\n",
    "# for subj, ts in tqdm(x_text_ts.items()):\n",
    "#     yt_hat[subj] = list()\n",
    "#     for text in ts:\n",
    "#         p = predict(text)\n",
    "#         yt_hat[subj].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_hat = pickle.load(open(\"stash/all.prediction.pkl\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING = 13\n",
    "YIELD_THRESHOLD = 0.990\n",
    "weight_window = np.ones(ROLLING) / ROLLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = list()\n",
    "for subj, ts in tqdm(yt_hat.items()):\n",
    "    T = len(ts)\n",
    "    result = 0\n",
    "    pred_window = np.array(yt_hat[subj][: ROLLING - 1] + [0])\n",
    "    for t in range(ROLLING - 1, T):\n",
    "        pred_window[t % ROLLING] = yt_hat[subj][t]\n",
    "        score = np.dot(pred_window, weight_window)\n",
    "        if score >= YIELD_THRESHOLD:\n",
    "            result = 1\n",
    "            break\n",
    "    y_hat.append([subj, result, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_mem(*list(zip(*[(y, y_true[subj], t) for subj, y, t in y_hat])), 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hypertuning?\n",
    "- explore refining methods on binary ts? HMM: https://pure.unileoben.ac.at/portal/files/1073252/Improving_Time_Series_Classification_Using_Hidden_Markov_Models.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(y_hat).to_csv(\"rollig_vote_results.csv\", index=False, header=None, sep=\" \")\n",
    "# erde(\"rollig_vote_results.csv\", 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## born"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember to set `HORIZON` and `SHIFT` both to `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(AAA_PATH + 'X_train.npy')\n",
    "y_train = np.load(AAA_PATH + 'y_train.npy')\n",
    "X_test = np.load(AAA_PATH + 'X_test.npy')\n",
    "y_test = np.load(AAA_PATH + 'y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bornrule import BornClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = 0.5 #amplitude\n",
    "balance = 1 #balance\n",
    "entropy = 1 #entropy\n",
    "\n",
    "born = BornClassifier(a=amplitude, b=balance, h=entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "born.fit(X = X_train.reshape(X_train.shape[:2]), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = born.predict(X_test.reshape(X_test.shape[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erde_mem(\n",
    "    y_hat,\n",
    "    y_test,\n",
    "    [1] * y_hat.shape[0],\n",
    "    5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIC SUNT LEONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0/0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from  statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = categories.index(\"family\")\n",
    "s = \"subject3414\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = x_sign_lex_ts[s][:50,k]\n",
    "ts = ma(ts, 3)\n",
    "ts = x_lex_ts[s][:50,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts, order=(1,3,2))\n",
    "results = model.fit()\n",
    "plt.plot(ts)\n",
    "plt.plot(results.fittedvalues[1:], color='red', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a03f457ba8100ed956a9dead337cf895f8e146c4ebc145127222e1f06f191e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
