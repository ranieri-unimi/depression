WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 64, 2000)     0
__________________________________________________________________________________________________
permute_1 (Permute)             (None, 2000, 64)     0           input_1[0][0]
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 2000, 128)    65664       permute_1[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 2000, 128)    512         conv1d_1[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 2000, 128)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_1 (Glo (None, 128)          0           activation_1[0][0]
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 1, 128)       0           global_average_pooling1d_1[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1, 8)         1024        reshape_1[0][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1, 128)       1024        dense_1[0][0]
__________________________________________________________________________________________________
multiply_1 (Multiply)           (None, 2000, 128)    0           activation_1[0][0]
                                                                 dense_2[0][0]
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 2000, 256)    164096      multiply_1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 2000, 256)    1024        conv1d_2[0][0]
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 2000, 256)    0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_2 (Glo (None, 256)          0           activation_2[0][0]
__________________________________________________________________________________________________
reshape_2 (Reshape)             (None, 1, 256)       0           global_average_pooling1d_2[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1, 16)        4096        reshape_2[0][0]
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1, 256)       4096        dense_3[0][0]
__________________________________________________________________________________________________
multiply_2 (Multiply)           (None, 2000, 256)    0           activation_2[0][0]
                                                                 dense_4[0][0]
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 2000, 128)    98432       multiply_2[0][0]
__________________________________________________________________________________________________
masking_1 (Masking)             (None, 64, 2000)     0           input_1[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 2000, 128)    512         conv1d_3[0][0]
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 8)            64288       masking_1[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 2000, 128)    0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 8)            0           lstm_1[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_3 (Glo (None, 128)          0           activation_3[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 136)          0           dropout_1[0][0]
                                                                 global_average_pooling1d_3[0][0]
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 2)            274         concatenate_1[0][0]
==================================================================================================
Total params: 405,042
Trainable params: 404,018
Non-trainable params: 1,024
__________________________________________________________________________________________________
Loading train / test dataset :  H:/git.riva/MLSTM_FCN/data/aaa/ H:/git.riva/MLSTM_FCN/data/aaa/
Finished processing train dataset..
Finished loading test dataset..
Number of train samples :  541 Number of test samples :  233
Sequence length :  2000
Class weights :  [0.54757085 5.75531915]
Train on 541 samples, validate on 233 samples
Epoch 1/5
 - 77s - loss: 0.5567 - accuracy: 0.7652 - val_loss: 0.6584 - val_accuracy: 0.8841

Epoch 00001: loss improved from inf to 0.55668, saving model to ./weights/aaa_weights.h5
Epoch 2/5
Number of train samples :  541 Number of test samples :  233
Number of classes :  2
Sequence length :  2000

Evaluating :
233/233 [==============================] - 10s 42ms/step

Final Accuracy :  0.8841201663017273
PS H:\git.riva>